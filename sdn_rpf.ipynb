{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f05d2fe5-39b3-4001-8261-cd067add0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca81b637-e12d-479e-bf68-efbd9e12e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternalClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_channels, num_classes, alpha=0.5):\n",
    "        super(InternalClassifier, self).__init__()\n",
    "        #red_kernel_size = -1 # to test the effects of the feature reduction\n",
    "        red_kernel_size = feature_reduction_formula(input_size) # get the pooling size\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        if red_kernel_size == -1:\n",
    "            self.linear = nn.Linear(output_channels*input_size*input_size, num_classes)\n",
    "            self.forward = self.forward_wo_pooling\n",
    "        else:\n",
    "            red_input_size = int(input_size/red_kernel_size)\n",
    "            self.max_pool = nn.MaxPool2d(kernel_size=red_kernel_size)\n",
    "            self.avg_pool = nn.AvgPool2d(kernel_size=red_kernel_size)\n",
    "            self.alpha = nn.Parameter(torch.rand(1))\n",
    "            self.linear = nn.Linear(output_channels*red_input_size*red_input_size, num_classes)\n",
    "            self.forward = self.forward_w_pooling\n",
    "\n",
    "    def forward_w_pooling(self, x):\n",
    "        avgp = self.alpha*self.max_pool(x)\n",
    "        maxp = (1 - self.alpha)*self.avg_pool(x)\n",
    "        mixed = avgp + maxp\n",
    "        return self.linear(mixed.view(mixed.size(0), -1))\n",
    "\n",
    "    def forward_wo_pooling(self, x):\n",
    "        return self.linear(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d40cf6-85e1-4786-9595-1e4809fb0824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdn_train(model, data, epochs, optimizer, scheduler, device='cpu'):\n",
    "    augment = model.augment_training\n",
    "    metrics = {'epoch_times':[], 'test_top1_acc':[], 'test_top5_acc':[], 'train_top1_acc':[], 'train_top5_acc':[], 'lrs':[]}\n",
    "    max_coeffs = np.array([0.15, 0.3, 0.45, 0.6, 0.75, 0.9]) # max tau_i --- C_i values\n",
    "\n",
    "    if model.ic_only:\n",
    "        print('sdn will be converted from a pre-trained CNN...  (The IC-only training)')\n",
    "    else:\n",
    "        print('sdn will be trained from scratch...(The SDN training)')\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        scheduler.step()\n",
    "        cur_lr = af.get_lr(optimizer)\n",
    "        print('\\nEpoch: {}/{}'.format(epoch, epochs))\n",
    "        print('Cur lr: {}'.format(cur_lr))\n",
    "\n",
    "        if model.ic_only is False:\n",
    "            # calculate the IC coeffs for this epoch for the weighted objective function\n",
    "            cur_coeffs = 0.01 + epoch*(max_coeffs/epochs) # to calculate the tau at the currect epoch\n",
    "            cur_coeffs = np.minimum(max_coeffs, cur_coeffs)\n",
    "            print('Cur coeffs: {}'.format(cur_coeffs))\n",
    "\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        loader = get_loader(data, augment)\n",
    "        for i, batch in enumerate(loader):\n",
    "            if model.ic_only is False:\n",
    "                total_loss = sdn_training_step(optimizer, model, cur_coeffs, batch, device)\n",
    "            else:\n",
    "                total_loss = sdn_ic_only_step(optimizer, model, batch, device)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Loss: {}: '.format(total_loss))\n",
    "\n",
    "        top1_test, top5_test = sdn_test(model, data.test_loader, device)\n",
    "\n",
    "        print('Top1 Test accuracies: {}'.format(top1_test))\n",
    "        print('Top5 Test accuracies: {}'.format(top5_test))\n",
    "        end_time = time.time()\n",
    "\n",
    "        metrics['test_top1_acc'].append(top1_test)\n",
    "        metrics['test_top5_acc'].append(top5_test)\n",
    "\n",
    "        top1_train, top5_train = sdn_test(model, get_loader(data, augment), device)\n",
    "        print('Top1 Train accuracies: {}'.format(top1_train))\n",
    "        print('Top5 Train accuracies: {}'.format(top5_train))\n",
    "        metrics['train_top1_acc'].append(top1_train)\n",
    "        metrics['train_top5_acc'].append(top5_train)\n",
    "\n",
    "        epoch_time = int(end_time-start_time)\n",
    "        metrics['epoch_times'].append(epoch_time)\n",
    "        print('Epoch took {} seconds.'.format(epoch_time))\n",
    "\n",
    "        metrics['lrs'].append(cur_lr)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554c23c1-a473-4a76-945d-95342d437a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlockWOutput(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, params, stride=1):\n",
    "        super(BasicBlockWOutput, self).__init__()\n",
    "        add_output = params[0]\n",
    "        num_classes = params[1]\n",
    "        input_size = params[2]\n",
    "        self.output_id = params[3]\n",
    "\n",
    "        self.depth = 2\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        conv_layer = []\n",
    "        conv_layer.append(nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
    "        conv_layer.append(nn.BatchNorm2d(channels))\n",
    "        conv_layer.append(nn.ReLU())\n",
    "        conv_layer.append(nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "        conv_layer.append(nn.BatchNorm2d(channels))\n",
    "\n",
    "        layers.append(nn.Sequential(*conv_layer))\n",
    "\n",
    "        shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != self.expansion*channels:\n",
    "            shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion*channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*channels)\n",
    "            )\n",
    "\n",
    "        layers.append(shortcut)\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        if add_output:\n",
    "            self.output = af.InternalClassifier(input_size, self.expansion*channels, num_classes) \n",
    "            self.no_output = False\n",
    "\n",
    "        else:\n",
    "            self.output = None\n",
    "            self.forward = self.only_forward\n",
    "            self.no_output = True\n",
    "            \n",
    "    def forward(self, x):\n",
    "        fwd = self.layers[0](x) # conv layers\n",
    "        fwd = fwd + self.layers[1](x) # shortcut\n",
    "        return self.layers[2](fwd), 1, self.output(fwd)         # output layers for this module\n",
    "    \n",
    "    def only_output(self, x):\n",
    "        fwd = self.layers[0](x) # conv layers\n",
    "        fwd = fwd + self.layers[1](x) # shortcut\n",
    "        fwd = self.layers[2](fwd) # activation\n",
    "        out = self.output(fwd)         # output layers for this module\n",
    "        return out\n",
    "    \n",
    "    def only_forward(self, x):\n",
    "        fwd = self.layers[0](x) # conv layers\n",
    "        fwd = fwd + self.layers[1](x) # shortcut\n",
    "        return self.layers[2](fwd), 0, None # activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "686d50f2-cf35-44db-b4b3-a928a816af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_SDN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(ResNet_SDN, self).__init__()\n",
    "        self.num_blocks = params['num_blocks']\n",
    "        self.num_classes = int(params['num_classes'])\n",
    "        self.augment_training = params['augment_training']\n",
    "        self.input_size = int(params['input_size'])\n",
    "        self.block_type = params['block_type']\n",
    "        self.add_out_nonflat = params['add_ic']\n",
    "        self.add_output = [item for sublist in self.add_out_nonflat for item in sublist]\n",
    "        self.init_weights = params['init_weights']\n",
    "        self.train_func = sdn_train\n",
    "        self.in_channels = 16\n",
    "        self.num_output = sum(self.add_output) + 1\n",
    "#        self.test_func = mf.sdn_test\n",
    "\n",
    "        self.init_depth = 1\n",
    "        self.end_depth = 1\n",
    "        self.cur_output_id = 0\n",
    "\n",
    "        if self.block_type == 'basic':\n",
    "            self.block = BasicBlockWOutput\n",
    "\n",
    "        init_conv = []\n",
    "\n",
    "        if self.input_size ==  32: # cifar10\n",
    "            self.cur_input_size = self.input_size\n",
    "            init_conv.append(nn.Conv2d(3, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "        else: # tiny imagenet\n",
    "            self.cur_input_size = int(self.input_size/2)\n",
    "            init_conv.append(nn.Conv2d(3, self.in_channels, kernel_size=3, stride=2, padding=1, bias=False))\n",
    "            \n",
    "        init_conv.append(nn.BatchNorm2d(self.in_channels))\n",
    "        init_conv.append(nn.ReLU())\n",
    "\n",
    "        self.init_conv = nn.Sequential(*init_conv)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend(self._make_layer(self.in_channels, block_id=0, stride=1))\n",
    "        \n",
    "        self.cur_input_size = int(self.cur_input_size/2)\n",
    "        self.layers.extend(self._make_layer(32, block_id=1, stride=2))\n",
    "        \n",
    "        self.cur_input_size = int(self.cur_input_size/2)\n",
    "        self.layers.extend(self._make_layer(64, block_id=2, stride=2))\n",
    "        \n",
    "        end_layers = []\n",
    "        \n",
    "        end_layers.append(nn.AvgPool2d(kernel_size=8))\n",
    "        end_layers.append(af.Flatten())\n",
    "        end_layers.append(nn.Linear(64*self.block.expansion, self.num_classes))\n",
    "        self.end_layers = nn.Sequential(*end_layers)\n",
    "\n",
    "        if self.init_weights:\n",
    "            self.initialize_weights()\n",
    "\n",
    "    def _make_layer(self, channels, block_id, stride):\n",
    "        num_blocks = int(self.num_blocks[block_id])\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for cur_block_id, stride in enumerate(strides):\n",
    "            add_output = self.add_out_nonflat[block_id][cur_block_id]\n",
    "            params  = (add_output, self.num_classes, int(self.cur_input_size), self.cur_output_id)\n",
    "            layers.append(self.block(self.in_channels, channels, params, stride))\n",
    "            self.in_channels = channels * self.block.expansion\n",
    "            self.cur_output_id += add_output\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        fwd = self.init_conv(x)\n",
    "        for layer in self.layers:\n",
    "            fwd, is_output, output = layer(fwd)\n",
    "            if is_output:\n",
    "                outputs.append(output)\n",
    "        fwd = self.end_layers(fwd)\n",
    "        outputs.append(fwd)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # takes a single input\n",
    "    def early_exit(self, x):\n",
    "        confidences = []\n",
    "        outputs = []\n",
    "\n",
    "        fwd = self.init_conv(x)\n",
    "        output_id = 0\n",
    "        for layer in self.layers:\n",
    "            fwd, is_output, output = layer(fwd)\n",
    "\n",
    "            if is_output:\n",
    "                outputs.append(output)\n",
    "                softmax = nn.functional.softmax(output[0], dim=0)\n",
    "                \n",
    "                confidence = torch.max(softmax)\n",
    "                confidences.append(confidence)\n",
    "            \n",
    "                if confidence >= self.confidence_threshold:\n",
    "                    is_early = True\n",
    "                    return output, output_id, is_early\n",
    "                \n",
    "                output_id += is_output\n",
    "\n",
    "        output = self.end_layers(fwd)\n",
    "        outputs.append(output)\n",
    "\n",
    "        softmax = nn.functional.softmax(output[0], dim=0)\n",
    "        confidence = torch.max(softmax)\n",
    "        confidences.append(confidence)\n",
    "        max_confidence_output = np.argmax(confidences)\n",
    "        is_early = False\n",
    "        return outputs[max_confidence_output], max_confidence_output, is_early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d6a3f6b-5556-409c-9161-ad28ca915e74",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'af' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m model_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mic_only\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmilestones\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m15\u001b[39m]\n\u001b[1;32m     25\u001b[0m model_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mic_only\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgammas\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mResNet_SDN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [11], line 39\u001b[0m, in \u001b[0;36mResNet_SDN.__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_conv \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39minit_conv)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_input_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_layer(\u001b[38;5;241m32\u001b[39m, block_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn [11], line 64\u001b[0m, in \u001b[0;36mResNet_SDN._make_layer\u001b[0;34m(self, channels, block_id, stride)\u001b[0m\n\u001b[1;32m     62\u001b[0m add_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_out_nonflat[block_id][cur_block_id]\n\u001b[1;32m     63\u001b[0m params  \u001b[38;5;241m=\u001b[39m (add_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_input_size), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_output_id)\n\u001b[0;32m---> 64\u001b[0m layers\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels \u001b[38;5;241m=\u001b[39m channels \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mexpansion\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_output_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_output\n",
      "Cell \u001b[0;32mIn [10], line 38\u001b[0m, in \u001b[0;36mBasicBlockWOutput.__init__\u001b[0;34m(self, in_channels, channels, params, stride)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m layers\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_output:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[43maf\u001b[49m\u001b[38;5;241m.\u001b[39mInternalClassifier(input_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpansion\u001b[38;5;241m*\u001b[39mchannels, num_classes) \n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'af' is not defined"
     ]
    }
   ],
   "source": [
    "model_params = {}\n",
    "model_params['task'] = 'cifar10'\n",
    "model_params['input_size'] = 32\n",
    "model_params['num_classes'] = 10\n",
    "model_params['block_type'] = 'basic'\n",
    "model_params['num_blocks'] = [9,9,9]\n",
    "model_params['add_ic'] = [[0, 0, 0, 1, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0, 0]] # 15, 30, 45, 60, 75, 90 percent of GFLOPs\n",
    "model_params['network_type'] = 'resnet56'\n",
    "model_params['augment_training'] = True\n",
    "model_params['init_weights'] = True\n",
    "model_params['architecture'] = 'sdn'\n",
    "model_params['base_model'] = 'resnet56'\n",
    "network_type = model_params['network_type']\n",
    "model_params['momentum'] = 0.9\n",
    "model_params['learning_rate'] = 0.1\n",
    "model_params['epochs'] = 100\n",
    "model_params['milestones'] = [35, 60, 85]\n",
    "model_params['gammas'] = [0.1, 0.1, 0.1]\n",
    "\n",
    "# SDN ic_only training params\n",
    "model_params['ic_only'] = {}\n",
    "model_params['ic_only']['learning_rate'] = 0.001 # lr for full network training after sdn modification\n",
    "model_params['ic_only']['epochs'] = 25\n",
    "model_params['ic_only']['milestones'] = [15]\n",
    "model_params['ic_only']['gammas'] = [0.1]\n",
    "model = ResNet_SDN(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4014dbe9-4fa9-4ec8-bb3e-eee01a2979d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a41ea-5bb3-47f0-b5ff-febca1ad050f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
