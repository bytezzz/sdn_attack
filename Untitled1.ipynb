{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "421e5689-9249-4514-aba5-991516ad6244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import aux_funcs  as af\n",
    "import network_architectures as arcs\n",
    "import model_funcs as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea39147d-2609-44fd-a9db-32727663b9a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBasicBlockWOutput\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     expansion \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_channels, channels, params, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class BasicBlockWOutput(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, params, stride=1):\n",
    "        super(BasicBlockWOutput, self).__init__()\n",
    "        add_output = params[0]\n",
    "        num_classes = params[1]\n",
    "        input_size = params[2]\n",
    "        self.output_id = params[3]\n",
    "\n",
    "        self.depth = 2\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        conv_layer = []\n",
    "        conv_layer.append(nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
    "        conv_layer.append(nn.BatchNorm2d(channels))\n",
    "        conv_layer.append(nn.ReLU())\n",
    "        conv_layer.append(nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "        conv_layer.append(nn.BatchNorm2d(channels))\n",
    "\n",
    "        layers.append(nn.Sequential(*conv_layer))\n",
    "\n",
    "        shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != self.expansion*channels:\n",
    "            shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion*channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*channels)\n",
    "            )\n",
    "\n",
    "        layers.append(shortcut)\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        if add_output:\n",
    "            self.output = af.InternalClassifier(input_size, self.expansion*channels, num_classes) \n",
    "            self.no_output = False\n",
    "\n",
    "        else:\n",
    "            self.output = None\n",
    "            self.forward = self.only_forward\n",
    "            self.no_output = True\n",
    "            \n",
    "    def forward(self, x):\n",
    "        fwd = self.layers[0](x) # conv layers\n",
    "        fwd = fwd + self.layers[1](x) # shortcut\n",
    "        return self.layers[2](fwd), 1, self.output(fwd)         # output layers for this module\n",
    "    \n",
    "    def only_output(self, x):\n",
    "        fwd = self.layers[0](x) # conv layers\n",
    "        fwd = fwd + self.layers[1](x) # shortcut\n",
    "        fwd = self.layers[2](fwd) # activation\n",
    "        out = self.output(fwd)         # output layers for this module\n",
    "        return out\n",
    "    \n",
    "    def only_forward(self, x):\n",
    "        fwd = self.layers[0](x) # conv layers\n",
    "        fwd = fwd + self.layers[1](x) # shortcut\n",
    "        return self.layers[2](fwd), 0, None # activation\n",
    "\n",
    "class ResNet_SDN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(ResNet_SDN, self).__init__()\n",
    "        self.num_blocks = params['num_blocks']\n",
    "        self.num_classes = int(params['num_classes'])\n",
    "        self.augment_training = params['augment_training']\n",
    "        self.input_size = int(params['input_size'])\n",
    "        self.block_type = params['block_type']\n",
    "        self.add_out_nonflat = params['add_ic']\n",
    "        self.add_output = [item for sublist in self.add_out_nonflat for item in sublist]\n",
    "        self.init_weights = params['init_weights']\n",
    "        self.train_func = mf.sdn_train\n",
    "        self.in_channels = 16\n",
    "        self.num_output = sum(self.add_output) + 1\n",
    "        self.test_func = mf.sdn_test\n",
    "\n",
    "        self.init_depth = 1\n",
    "        self.end_depth = 1\n",
    "        self.cur_output_id = 0\n",
    "        \n",
    "        self.stop_ic_id = -1\n",
    "\n",
    "        if self.block_type == 'basic':\n",
    "            self.block = BasicBlockWOutput\n",
    "\n",
    "        init_conv = []\n",
    "\n",
    "        if self.input_size ==  32: # cifar10\n",
    "            self.cur_input_size = self.input_size\n",
    "            init_conv.append(nn.Conv2d(3, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "        else: # tiny imagenet\n",
    "            self.cur_input_size = int(self.input_size/2)\n",
    "            init_conv.append(nn.Conv2d(3, self.in_channels, kernel_size=3, stride=2, padding=1, bias=False))\n",
    "            \n",
    "        init_conv.append(nn.BatchNorm2d(self.in_channels))\n",
    "        init_conv.append(nn.ReLU())\n",
    "\n",
    "        self.init_conv = nn.Sequential(*init_conv)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend(self._make_layer(self.in_channels, block_id=0, stride=1))\n",
    "        \n",
    "        self.cur_input_size = int(self.cur_input_size/2)\n",
    "        self.layers.extend(self._make_layer(32, block_id=1, stride=2))\n",
    "        \n",
    "        self.cur_input_size = int(self.cur_input_size/2)\n",
    "        self.layers.extend(self._make_layer(64, block_id=2, stride=2))\n",
    "        \n",
    "        end_layers = []\n",
    "        \n",
    "        end_layers.append(nn.AvgPool2d(kernel_size=8))\n",
    "        end_layers.append(af.Flatten())\n",
    "        end_layers.append(nn.Linear(64*self.block.expansion, self.num_classes))\n",
    "        self.end_layers = nn.Sequential(*end_layers)\n",
    "\n",
    "        if self.init_weights:\n",
    "            self.initialize_weights()\n",
    "\n",
    "    def _make_layer(self, channels, block_id, stride):\n",
    "        num_blocks = int(self.num_blocks[block_id])\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for cur_block_id, stride in enumerate(strides):\n",
    "            add_output = self.add_out_nonflat[block_id][cur_block_id]\n",
    "            params  = (add_output, self.num_classes, int(self.cur_input_size), self.cur_output_id)\n",
    "            layers.append(self.block(self.in_channels, channels, params, stride))\n",
    "            self.in_channels = channels * self.block.expansion\n",
    "            self.cur_output_id += add_output\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        fwd = self.init_conv(x)\n",
    "        for layer in self.layers:\n",
    "            fwd, is_output, output = layer(fwd)\n",
    "            if is_output:\n",
    "                if len(outputs) == self.stop_ic_id:\n",
    "                    return output\n",
    "                outputs.append(output)\n",
    "        fwd = self.end_layers(fwd)\n",
    "        outputs.append(fwd)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # takes a single input\n",
    "    def early_exit(self, x):\n",
    "        confidences = []\n",
    "        outputs = []\n",
    "\n",
    "        fwd = self.init_conv(x)\n",
    "        output_id = 0\n",
    "        for layer in self.layers:\n",
    "            fwd, is_output, output = layer(fwd)\n",
    "\n",
    "            if is_output:\n",
    "                outputs.append(output)\n",
    "                softmax = nn.functional.softmax(output[0], dim=0)\n",
    "                \n",
    "                confidence = torch.max(softmax)\n",
    "                confidences.append(confidence)\n",
    "            \n",
    "                if confidence >= self.confidence_threshold:\n",
    "                    is_early = True\n",
    "                    return output, output_id, is_early\n",
    "                \n",
    "                output_id += is_output\n",
    "\n",
    "        output = self.end_layers(fwd)\n",
    "        outputs.append(output)\n",
    "\n",
    "        softmax = nn.functional.softmax(output[0], dim=0)\n",
    "        confidence = torch.max(softmax)\n",
    "        confidences.append(confidence)\n",
    "        max_confidence_output = np.argmax(confidences)\n",
    "        is_early = False\n",
    "        return outputs[max_confidence_output], max_confidence_output, is_early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47fdf893-9e34-4b3c-b0b1-03d6c2b1ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {}\n",
    "model_params['task'] = 'cifar10'\n",
    "model_params['input_size'] = 32\n",
    "model_params['num_classes'] = 10\n",
    "model_params['block_type'] = 'basic'\n",
    "model_params['num_blocks'] = [9,9,9]\n",
    "model_params['add_ic'] = [[0, 0, 0, 1, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0, 0]] # 15, 30, 45, 60, 75, 90 percent of GFLOPs\n",
    "model_params['network_type'] = 'resnet56'\n",
    "model_params['augment_training'] = True\n",
    "model_params['init_weights'] = True\n",
    "model_params['architecture'] = 'sdn'\n",
    "model_params['base_model'] = 'resnet56'\n",
    "network_type = model_params['network_type']\n",
    "model_params['momentum'] = 0.9\n",
    "model_params['learning_rate'] = 0.1\n",
    "model_params['epochs'] = 100\n",
    "model_params['milestones'] = [35, 60, 85]\n",
    "model_params['gammas'] = [0.1, 0.1, 0.1]\n",
    "\n",
    "# SDN ic_only training params\n",
    "model_params['ic_only'] = {}\n",
    "model_params['ic_only']['learning_rate'] = 0.001 # lr for full network training after sdn modification\n",
    "model_params['ic_only']['epochs'] = 25\n",
    "model_params['ic_only']['milestones'] = [15]\n",
    "model_params['ic_only']['gammas'] = [0.1]\n",
    "model = ResNet_SDN(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c265f1-f0a2-413f-9119-0ea69c1067ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstop_ic_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      2\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.stop_ic_id = 1\n",
    "batch_shape = [128, 3, 32, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef35e95-cfba-44fe-b87c-4e864d641cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f942c3d-4ea3-477c-a100-5c748ef280b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attack(model,layer_id, test_loader, atk, atk_name):\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    n = 0\n",
    "    model.eval()\n",
    "    model.stop_ic_id = layer_id\n",
    "    for i, (X, y) in enumerate(test_loader):\n",
    "        X, y = X.to('cuda'), y.to('cuda')\n",
    "\n",
    "        #if args.rp:\n",
    "            # random select a path to attack\n",
    "        #    model.module.random_rp_matrix()\n",
    "\n",
    "        X_adv = atk(X, y)  # advtorch\n",
    "\n",
    "        #if args.rp:\n",
    "         #   # random select a path to infer\n",
    "          #  model.module.random_rp_matrix()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(X_adv)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        test_loss += loss.item() * y.size(0)\n",
    "        test_acc += (output.max(1)[1] == y).sum().item()\n",
    "        n += y.size(0)\n",
    "\n",
    "    pgd_acc = test_acc / n\n",
    "    print('Attack_type: [{:s}] done, acc: {:.4f} \\t'.format(atk_name, pgd_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1ae36a0-3e93-435b-a2ee-6646064e333b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar = af.get_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f63edf0-a3a0-4ef8-8cb2-c51aed5a315f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack_type: [fgsm] done, acc: 0.0026 \t\n"
     ]
    }
   ],
   "source": [
    "import torchattacks\n",
    "atk = torchattacks.FGSM(model, eps=8/255)\n",
    "atk.set_normalization_used(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "evaluate_attack(model,1, cifar.test_loader, atk, 'fgsm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c3a03-4094-4ae1-a5ce-e8429b897b0c",
   "metadata": {},
   "source": [
    "python train_imagenet.py --pretrained --lr 0.02 --lr_schedule cosine --batch_size 1024 --epochs 90 --adv_train --rp --rp_block -1 -1 --rp_out_channel 48 --rp_weight_decay 1e-2 --save_dir resnet50_imagenet_RPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82db9285-7f24-4b78-b240-4800da278bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlockWOutput(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, params, stride=1):\n",
    "        super(BasicBlockWOutput, self).__init__()\n",
    "        add_output = params[0]\n",
    "        num_classes = params[1]\n",
    "        input_size = params[2]\n",
    "        self.output_id = params[3]\n",
    "\n",
    "        self.depth = 2\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        conv_layer = []\n",
    "        conv_layer.append(nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
    "        conv_layer.append(nn.BatchNorm2d(channels))\n",
    "        conv_layer.append(nn.ReLU())\n",
    "        conv_layer.append(nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "        conv_layer.append(nn.BatchNorm2d(channels))\n",
    "\n",
    "        layers.append(nn.Sequential(*conv_layer))\n",
    "\n",
    "        shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != self.expansion*channels:\n",
    "            shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion*channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*channels)\n",
    "            )\n",
    "\n",
    "        layers.append(shortcut)\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        if add_output:\n",
    "            self.output = af.InternalClassifier(input_size, self.expansion*channels, num_classes) \n",
    "            self.no_output = False\n",
    "\n",
    "        else:\n",
    "            self.output = None\n",
    "            self.forward = self.only_forward\n",
    "            self.no_output = True\n",
    "            \n",
    "    def forward(self, x):\n",
    "        fwd = self.layers[0](x) # conv layers\n",
    "        fwd = fwd + self.layers[1](x) # shortcut\n",
    "        return self.layers[2](fwd), 1, self.output(fwd)         # output layers for this module\n",
    "    \n",
    "    def only_output(self, x):\n",
    "        fwd = self.layers[0](x) # conv layers\n",
    "        fwd = fwd + self.layers[1](x) # shortcut\n",
    "        fwd = self.layers[2](fwd) # activation\n",
    "        out = self.output(fwd)         # output layers for this module\n",
    "        return out\n",
    "    \n",
    "    def only_forward(self, x):\n",
    "        fwd = self.layers[0](x) # conv layers\n",
    "        fwd = fwd + self.layers[1](x) # shortcut\n",
    "        return self.layers[2](fwd), 0, None # activation\n",
    "\n",
    "class ResNet_SDN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(ResNet_SDN, self).__init__()\n",
    "        self.num_blocks = params['num_blocks']\n",
    "        self.num_classes = int(params['num_classes'])\n",
    "        self.augment_training = params['augment_training']\n",
    "        self.input_size = int(params['input_size'])\n",
    "        self.block_type = params['block_type']\n",
    "        self.add_out_nonflat = params['add_ic']\n",
    "        self.add_output = [item for sublist in self.add_out_nonflat for item in sublist]\n",
    "        self.init_weights = params['init_weights']\n",
    "        self.train_func = mf.sdn_train\n",
    "        self.in_channels = 16\n",
    "        self.num_output = sum(self.add_output) + 1\n",
    "        self.test_func = mf.sdn_test\n",
    "\n",
    "        self.init_depth = 1\n",
    "        self.end_depth = 1\n",
    "        self.cur_output_id = 0\n",
    "        \n",
    "        self.stop_ic_id = -1\n",
    "        \n",
    "        self.init_rpf_channels = params['init_rpf_pannel']\n",
    "        self.use_rpf = params['use_rpf']\n",
    "\n",
    "        if self.block_type == 'basic':\n",
    "            self.block = BasicBlockWOutput\n",
    "\n",
    "        init_conv = []\n",
    "\n",
    "        if self.input_size ==  32: # cifar10\n",
    "            self.cur_input_size = self.input_size\n",
    "            if self.use_rpf:\n",
    "                init_conv.append(nn.Conv2d(3, self.in_channels - self.init_rpf_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "                init_conv.append(nn.Conv2d(3, self.init_rpf_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "            else:\n",
    "                init_conv.append(nn.Conv2d(3, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "        else: # tiny imagenet\n",
    "            self.cur_input_size = int(self.input_size/2)\n",
    "            if self.use_rpf:\n",
    "                init_conv.append(nn.Conv2d(3, self.in_channels - self.init_rpf_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "                init_conv.append(nn.Conv2d(3, self.init_rpf_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "            else:\n",
    "                init_conv.append(nn.Conv2d(3, self.in_channels, kernel_size=3, stride=2, padding=1, bias=False))\n",
    "        \n",
    "        init_conv.append(nn.BatchNorm2d(self.in_channels))\n",
    "        init_conv.append(nn.ReLU())\n",
    "\n",
    "        self.init_conv = nn.Sequential(*init_conv)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend(self._make_layer(self.in_channels, block_id=0, stride=1))\n",
    "        \n",
    "        self.cur_input_size = int(self.cur_input_size/2)\n",
    "        self.layers.extend(self._make_layer(32, block_id=1, stride=2))\n",
    "        \n",
    "        self.cur_input_size = int(self.cur_input_size/2)\n",
    "        self.layers.extend(self._make_layer(64, block_id=2, stride=2))\n",
    "        \n",
    "        end_layers = []\n",
    "        \n",
    "        end_layers.append(nn.AvgPool2d(kernel_size=8))\n",
    "        end_layers.append(af.Flatten())\n",
    "        end_layers.append(nn.Linear(64*self.block.expansion, self.num_classes))\n",
    "        self.end_layers = nn.Sequential(*end_layers)\n",
    "\n",
    "        if self.init_weights:\n",
    "            self.initialize_weights()\n",
    "\n",
    "    def _make_layer(self, channels, block_id, stride):\n",
    "        num_blocks = int(self.num_blocks[block_id])\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for cur_block_id, stride in enumerate(strides):\n",
    "            add_output = self.add_out_nonflat[block_id][cur_block_id]\n",
    "            params  = (add_output, self.num_classes, int(self.cur_input_size), self.cur_output_id)\n",
    "            layers.append(self.block(self.in_channels, channels, params, stride))\n",
    "            self.in_channels = channels * self.block.expansion\n",
    "            self.cur_output_id += add_output\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                                 \n",
    "    def random_rp_matrix(self):\n",
    "        param = next(self.init_conv[0].parameters())\n",
    "        kernel_size = param.data.size()[-1]\n",
    "        param.data = torch.normal(mean=0.0, std=1/kernel_size, size=param.data.size()).to('cuda')\n",
    "        \n",
    "    def rp_forward(self, x, out, kernel):\n",
    "        rp_out = kernel(x)\n",
    "        if out is None:\n",
    "            return rp_out\n",
    "        else:\n",
    "            out = torch.cat([out, rp_out], dim=1)\n",
    "            return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        if self.use_rpf:\n",
    "            fwd = self.init_conv[0](x)\n",
    "            fwd = self.rp_forward(x, fwd, self.init_conv[1])\n",
    "        fwd = self.init_conv[2](fwd)\n",
    "        for layer in self.layers:\n",
    "            fwd, is_output, output = layer(fwd)\n",
    "            if is_output:\n",
    "                if len(outputs) == self.stop_ic_id:\n",
    "                    return output\n",
    "                outputs.append(output)\n",
    "        fwd = self.end_layers(fwd)\n",
    "        outputs.append(fwd)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # takes a single input\n",
    "    def early_exit(self, x):\n",
    "        confidences = []\n",
    "        outputs = []\n",
    "\n",
    "        fwd = self.init_conv(x)\n",
    "        output_id = 0\n",
    "        for layer in self.layers:\n",
    "            fwd, is_output, output = layer(fwd)\n",
    "\n",
    "            if is_output:\n",
    "                outputs.append(output)\n",
    "                softmax = nn.functional.softmax(output[0], dim=0)\n",
    "                \n",
    "                confidence = torch.max(softmax)\n",
    "                confidences.append(confidence)\n",
    "            \n",
    "                if confidence >= self.confidence_threshold:\n",
    "                    is_early = True\n",
    "                    return output, output_id, is_early\n",
    "                \n",
    "                output_id += is_output\n",
    "\n",
    "        output = self.end_layers(fwd)\n",
    "        outputs.append(output)\n",
    "\n",
    "        softmax = nn.functional.softmax(output[0], dim=0)\n",
    "        confidence = torch.max(softmax)\n",
    "        confidences.append(confidence)\n",
    "        max_confidence_output = np.argmax(confidences)\n",
    "        is_early = False\n",
    "        return outputs[max_confidence_output], max_confidence_output, is_early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5164ff82-2e34-4136-8d07-d2a830636478",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {}\n",
    "model_params['task'] = 'cifar10'\n",
    "model_params['input_size'] = 32\n",
    "model_params['num_classes'] = 10\n",
    "model_params['block_type'] = 'basic'\n",
    "model_params['num_blocks'] = [9,9,9]\n",
    "model_params['add_ic'] = [[0, 0, 0, 1, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0, 0]] # 15, 30, 45, 60, 75, 90 percent of GFLOPs\n",
    "model_params['network_type'] = 'resnet56'\n",
    "model_params['augment_training'] = True\n",
    "model_params['init_weights'] = True\n",
    "model_params['architecture'] = 'sdn'\n",
    "model_params['base_model'] = 'resnet56'\n",
    "network_type = model_params['network_type']\n",
    "model_params['momentum'] = 0.9\n",
    "model_params['learning_rate'] = 0.1\n",
    "model_params['epochs'] = 100\n",
    "model_params['milestones'] = [35, 60, 85]\n",
    "model_params['gammas'] = [0.1, 0.1, 0.1]\n",
    "                          \n",
    "model_params['init_rpf_pannel'] = 4\n",
    "model_params['use_rpf'] = True\n",
    "\n",
    "# SDN ic_only training params\n",
    "model_params['ic_only'] = {}\n",
    "model_params['ic_only']['learning_rate'] = 0.001 # lr for full network training after sdn modification\n",
    "model_params['ic_only']['epochs'] = 25\n",
    "model_params['ic_only']['milestones'] = [15]\n",
    "model_params['ic_only']['gammas'] = [0.1]\n",
    "model = ResNet_SDN(model_params).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3a18135a-fabe-44b3-9908-efea2fc457e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.random_rp_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c3f5559-eb3d-4a84-9905-577749b8a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shape = [128, 3, 32, 32]\n",
    "test_input = torch.rand(batch_shape).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2267f185-2b1c-4a5e-8770-7e21d101c43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.7699, -1.6509,  1.4264,  ...,  3.6097,  1.0541, -1.1892],\n",
       "         [ 1.3752, -1.9357,  2.3942,  ...,  2.7096,  1.0237, -1.3450],\n",
       "         [ 1.4939, -1.6704,  1.8561,  ...,  2.3412,  0.9103, -1.4915],\n",
       "         ...,\n",
       "         [ 1.1259, -1.5833,  1.6241,  ...,  2.8987,  1.3286, -0.8291],\n",
       "         [ 1.9926, -1.4004,  2.4155,  ...,  2.5880,  1.1130, -0.9591],\n",
       "         [ 2.3156, -1.5465,  2.4426,  ...,  3.1357,  1.4794, -1.7622]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 2.3422,  0.9239, -1.4139,  ..., -3.4572,  4.0040, -4.8875],\n",
       "         [ 3.0376,  1.5566, -1.8930,  ..., -4.0567,  4.1819, -4.9462],\n",
       "         [ 3.5187,  0.9601, -1.6615,  ..., -3.5721,  4.0404, -4.5419],\n",
       "         ...,\n",
       "         [ 2.9644,  0.8580, -1.3493,  ..., -2.6545,  4.4944, -5.5623],\n",
       "         [ 3.4724,  1.0980, -1.0595,  ..., -3.4149,  3.4536, -4.2059],\n",
       "         [ 2.4395,  0.9000, -1.4464,  ..., -3.2744,  4.4771, -4.8074]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 0.2863, -0.2166, -0.9652,  ...,  3.1622,  0.2044, -0.6718],\n",
       "         [ 0.7710,  0.2444, -0.1537,  ...,  3.1016, -0.4512, -1.3959],\n",
       "         [ 1.0476,  0.6581,  0.0187,  ...,  3.6865,  0.2512, -1.7060],\n",
       "         ...,\n",
       "         [ 1.3400, -0.2202, -0.3632,  ...,  3.5944,  0.5719, -0.5135],\n",
       "         [ 0.0835,  0.1759, -0.3072,  ...,  3.6237,  0.2851, -0.9383],\n",
       "         [ 0.6021, -0.0985, -0.6064,  ...,  2.5979,  0.4453, -1.2574]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4728,  0.2302,  1.1876,  ...,  2.5628,  2.9700,  0.3349],\n",
       "         [-2.2425, -0.4617,  1.8730,  ...,  2.7519,  1.8552, -1.8823],\n",
       "         [-2.2906,  0.0808,  1.7883,  ...,  2.1954,  1.0213, -2.2154],\n",
       "         ...,\n",
       "         [-2.1527,  0.7185,  1.2666,  ...,  2.9602,  3.4912, -1.4767],\n",
       "         [-1.8581, -0.0859,  2.2283,  ...,  2.6783,  2.3584, -2.8414],\n",
       "         [-1.8699, -0.7782,  2.0005,  ...,  2.8222,  3.8772, -0.7209]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.4559,  0.8239,  0.2747,  ...,  0.9154,  0.6594, -0.4470],\n",
       "         [-0.5640,  1.3176,  0.1713,  ...,  1.4540,  0.6493, -0.8833],\n",
       "         [-1.5854,  0.0771,  1.0758,  ...,  1.0259,  0.3485, -1.2373],\n",
       "         ...,\n",
       "         [-0.3197,  0.7169,  0.3071,  ...,  1.1569,  0.1256, -1.0649],\n",
       "         [ 0.5886,  0.9465,  0.1259,  ...,  0.6331,  0.3578, -0.7476],\n",
       "         [-0.5328,  0.5756,  0.4240,  ...,  0.8799,  0.6459, -1.1667]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 0.2584,  1.5458, -1.2195,  ..., -0.1847, -1.8272,  0.0801],\n",
       "         [ 0.3456,  0.8167, -0.8641,  ..., -1.6977, -3.1728, -2.2728],\n",
       "         [ 0.0921,  1.5489,  0.3328,  ..., -2.4030, -2.2633, -1.4796],\n",
       "         ...,\n",
       "         [-0.6561,  0.0844,  0.1968,  ..., -2.2175, -2.5915, -0.5575],\n",
       "         [ 0.1907,  0.5509, -0.3983,  ..., -1.3896, -2.3491, -0.6957],\n",
       "         [ 0.6006,  1.5039, -0.3331,  ..., -0.2679, -2.3854, -0.6375]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.0563,  1.4586,  1.0246,  ..., -1.1255, -1.2493,  2.0806],\n",
       "         [ 0.0126,  1.3723,  1.2550,  ..., -1.2944, -1.4654,  2.3629],\n",
       "         [ 0.0500,  1.3054,  1.1554,  ..., -1.3087, -1.4940,  2.2584],\n",
       "         ...,\n",
       "         [ 0.2262,  1.3692,  1.1865,  ..., -0.9813, -1.6887,  2.3648],\n",
       "         [ 0.1094,  1.3574,  1.0146,  ..., -1.0857, -1.3986,  2.0363],\n",
       "         [-0.1040,  1.4441,  1.3208,  ..., -1.1300, -1.4595,  2.3354]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward0>)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a429bc-7573-4dd5-9a43-97aace6021fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
